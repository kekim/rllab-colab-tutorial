{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"day1_rllab_2_dqn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"isn8nKhR1Nyh","colab_type":"code","colab":{}},"cell_type":"code","source":["from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()\n","import os\n","os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IGfRFSAFC4Q5","colab_type":"text"},"cell_type":"markdown","source":["# DQN on GridWorld\n","**Important!! ** Before running the following cell, make sure rllab is set up properly in your **current** runtime by executing codes in **day1_rllab_0_setup.ipynb** \n","\n","Also, when run for the first time, the code will exit without training, just creating the personal profile. If this happens, just run the code again."]},{"metadata":{"id":"667frt61Ctzo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"230d1f71-a81b-48a7-81ca-57f9c2e1ea7a","executionInfo":{"status":"ok","timestamp":1546860253677,"user_tz":-540,"elapsed":37270,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["from rllab.envs.grid_world_env import GridWorldEnv\n","from dqn.envs.proxy_gym_env import ProxyGymEnv\n","from dqn.misc.retro_wrappers import wrap_deepmind_retro\n","from dqn.policies.categorical_mlp_q_policy import CategoricalMlpQPolicy\n","from dqn.exploration_strategies.eps_greedy_strategy import EpsilonGreedyStrategy\n","\n","from dqn.algos.dqn import DQN\n","\n","from rllab.envs.normalized_env import normalize\n","from rllab.misc.instrument import run_experiment_lite\n","from rllab.q_functions.continuous_mlp_q_function import ContinuousMLPQFunction\n","\n","import lasagne.nonlinearities as NL\n","\n","\n","def run_task(*_):\n","    env = GridWorldEnv(desc='chain')\n","    \n","    policy = CategoricalMlpQPolicy(\n","        name='dqn_policy',\n","        env_spec=env.spec,\n","        # The neural network policy should have two hidden layers, each with 32 hidden units.\n","        hidden_sizes=[],\n","    )\n","    \n","    n_steps = 40000\n","    es = EpsilonGreedyStrategy(env_spec=env.spec, max_eps=0.5, min_eps=0.05, decay_period=n_steps//2)\n","    \n","    algo = DQN(\n","        env=env,\n","        policy=policy,\n","        es=es,\n","        n_steps=n_steps,\n","        min_pool_size=50,\n","        replay_pool_size=100,\n","        train_epoch_interval=1000,\n","        max_path_length=50,\n","        policy_update_method='sgd',\n","        policy_learning_rate=0.2,\n","        target_model_update=0.5,\n","        n_eval_samples=0,\n","        batch_size=10,\n","        # Uncomment both lines (this and the plot parameter below) to enable plotting\n","        # plot=True,\n","    )\n","    algo.train()\n","\n","# run_task()\n","  \n","   \n","run_experiment_lite(\n","    run_task,\n","    log_dir='./gridworld_dqn',\n","    # Number of parallel workers for sampling\n","    n_parallel=1,\n","    # Only keep the snapshot parameters for the last iteration\n","    snapshot_mode=\"last\",\n","    # Specifies the seed for the experiment. If this is not provided, a random seed\n","    # will be used\n","    seed=1,\n","    # plot=True,\n",")\n","\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["python /content/scripts/run_experiment_lite.py  --n_parallel '1'  --snapshot_mode 'last'  --seed '1'  --exp_name 'experiment_2019_01_07_11_17_57_0002'  --log_dir './gridworld_dqn'  --use_cloudpickle 'True'  --args_data 'gASVJAQAAAAAAACMF2Nsb3VkcGlja2xlLmNsb3VkcGlja2xllIwOX2ZpbGxfZnVuY3Rpb26Uk5QoaACMD19tYWtlX3NrZWxfZnVuY5STlGgAjA1fYnVpbHRpbl90eXBllJOUjAhDb2RlVHlwZZSFlFKUKEsASwBLBksPS0dDYnQAZAFkAo0BfQF0AWQDfAFqAmcAZASNA30CZAV9A3QDfAFqAmQGZAd8A2QIGgBkCY0EfQR0BHwBfAJ8BHwDZApkC2QMZApkDWQOZAZkD2QQZBGNDX0FfAVqBYMAAQBkAFMAlChOjAVjaGFpbpSMBGRlc2OUhZSMCmRxbl9wb2xpY3mUjARuYW1llIwIZW52X3NwZWOUjAxoaWRkZW5fc2l6ZXOUh5RNQJxHP+AAAAAAAABHP6mZmZmZmZpLAihoEIwHbWF4X2Vwc5SMB21pbl9lcHOUjAxkZWNheV9wZXJpb2SUdJRLMktkTegDjANzZ2SURz/JmZmZmZmaSwBLCiiMA2VudpSMBnBvbGljeZSMAmVzlIwHbl9zdGVwc5SMDW1pbl9wb29sX3NpemWUjBByZXBsYXlfcG9vbF9zaXpllIwUdHJhaW5fZXBvY2hfaW50ZXJ2YWyUjA9tYXhfcGF0aF9sZW5ndGiUjBRwb2xpY3lfdXBkYXRlX21ldGhvZJSMFHBvbGljeV9sZWFybmluZ19yYXRllIwTdGFyZ2V0X21vZGVsX3VwZGF0ZZSMDm5fZXZhbF9zYW1wbGVzlIwKYmF0Y2hfc2l6ZZR0lHSUKIwMR3JpZFdvcmxkRW52lIwVQ2F0ZWdvcmljYWxNbHBRUG9saWN5lIwEc3BlY5SMFUVwc2lsb25HcmVlZHlTdHJhdGVneZSMA0RRTpSMBXRyYWlulHSUKIwBX5RoGGgZaBtoGowEYWxnb5R0lIwePGlweXRob24taW5wdXQtNS00ZjUwNWRhOTJiZTI+lIwIcnVuX3Rhc2uUSxBDLAABCgICAQIBBAIIAwQBFgICAQIBAgECAQIBAgECAQIBAgECAQIBAgECAQgElCkpdJRSlEr/////jAhfX21haW5fX5SHlFKUfZQojAdnbG9iYWxzlH2UKGgojCVkcW4ucG9saWNpZXMuY2F0ZWdvcmljYWxfbWxwX3FfcG9saWN5lGgok5RoJ4wZcmxsYWIuZW52cy5ncmlkX3dvcmxkX2VudpRoJ5OUaCuMDWRxbi5hbGdvcy5kcW6UaCuTlGgqjC5kcW4uZXhwbG9yYXRpb25fc3RyYXRlZ2llcy5lcHNfZ3JlZWR5X3N0cmF0ZWd5lGgqk5R1jAhkZWZhdWx0c5ROjARkaWN0lH2UjA5jbG9zdXJlX3ZhbHVlc5ROjAZtb2R1bGWUaDZoD2gyjANkb2OUTowIcXVhbG5hbWWUaDJ1dFIu'  --variant_data 'gAN9cQBYCAAAAGV4cF9uYW1lcQFYIwAAAGV4cGVyaW1lbnRfMjAxOV8wMV8wN18xMV8xN181N18wMDAycQJzLg=='\n"],"name":"stdout"}]},{"metadata":{"id":"H6rtQMrJq4y5","colab_type":"text"},"cell_type":"markdown","source":["# DQN on CartPole (OpenAI Gym version)\n","\n","**Important!! ** Before running the following cell, make sure rllab is set up properly in your **current** runtime by executing codes in **day1_rllab_0_setup.ipynb** \n","\n","Also, when run for the first time, the code will exit without training, just creating the personal profile. If this happens, just run the code again."]},{"metadata":{"id":"jVdzZRiciemp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"8fe51708-5d67-4e7f-ed61-e897533f1ce2","executionInfo":{"status":"ok","timestamp":1546859986345,"user_tz":-540,"elapsed":106945,"user":{"displayName":"Keeeung Kim","photoUrl":"","userId":"02647300226657852643"}}},"cell_type":"code","source":["from rllab.envs.gym_env import GymEnv\n","from dqn.policies.categorical_mlp_q_policy import CategoricalMlpQPolicy\n","from dqn.exploration_strategies.eps_greedy_strategy import EpsilonGreedyStrategy\n","\n","from dqn.algos.dqn import DQN\n","\n","from rllab.envs.normalized_env import normalize\n","from rllab.misc.instrument import run_experiment_lite\n","from rllab.q_functions.continuous_mlp_q_function import ContinuousMLPQFunction\n","\n","import lasagne.nonlinearities as NL\n","\n","\n","def run_task(*_):\n","    env = GymEnv('CartPole-v0', record_video=False)\n","    \n","    policy = CategoricalMlpQPolicy(\n","        name='dqn_policy',\n","        env_spec=env.spec,\n","        # The neural network policy should have two hidden layers, each with 32 hidden units.\n","        hidden_sizes=[64],\n","        hidden_nonlinearity=NL.rectify\n","    )\n","    \n","    n_steps = 80000\n","    es = EpsilonGreedyStrategy(env_spec=env.spec, max_eps=0.5, min_eps=0.05, decay_period=n_steps//4)\n","    \n","    algo = DQN(\n","        env=env,\n","        policy=policy,\n","        es=es,\n","        n_steps=n_steps,\n","        min_pool_size=100,\n","        replay_pool_size=200,\n","        train_epoch_interval=1000,\n","        max_path_length=200,\n","        policy_update_method='sgd',\n","        policy_learning_rate=0.0005,\n","        target_model_update=0.5,\n","        n_eval_samples=0,\n","        batch_size=20,\n","        # Uncomment both lines (this and the plot parameter below) to enable plotting\n","        # plot=True,\n","    )\n","    algo.train()\n","\n","# run_task()\n"," \n","  \n","run_experiment_lite(\n","    run_task,\n","    log_dir='./cartpole_dqn',\n","    # Number of parallel workers for sampling\n","    n_parallel=1,\n","    # Only keep the snapshot parameters for the last iteration\n","    snapshot_mode=\"last\",\n","    # Specifies the seed for the experiment. If this is not provided, a random seed\n","    # will be used\n","    seed=1,\n","    # plot=True,\n",")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["python /content/scripts/run_experiment_lite.py  --n_parallel '1'  --snapshot_mode 'last'  --seed '1'  --exp_name 'experiment_2019_01_07_11_17_57_0001'  --log_dir './cartpole_dqn'  --use_cloudpickle 'True'  --args_data 'gASViQQAAAAAAACMF2Nsb3VkcGlja2xlLmNsb3VkcGlja2xllIwOX2ZpbGxfZnVuY3Rpb26Uk5QoaACMD19tYWtlX3NrZWxfZnVuY5STlGgAjA1fYnVpbHRpbl90eXBllJOUjAhDb2RlVHlwZZSFlFKUKEsASwBLBksPS0dDanQAZAFkAmQDjQJ9AXQBZAR8AWoCZAVnAXQDagRkBo0EfQJkB30DdAV8AWoCZAhkCXwDZAoaAGQLjQR9BHQGfAF8AnwEfANkDGQNZA5kDWQPZBBkCGQRZBJkE40NfQV8BWoHgwABAGQAUwCUKE6MC0NhcnRQb2xlLXYwlImMDHJlY29yZF92aWRlb5SFlIwKZHFuX3BvbGljeZRLQCiMBG5hbWWUjAhlbnZfc3BlY5SMDGhpZGRlbl9zaXplc5SME2hpZGRlbl9ub25saW5lYXJpdHmUdJRKgDgBAEc/4AAAAAAAAEc/qZmZmZmZmksEKGgQjAdtYXhfZXBzlIwHbWluX2Vwc5SMDGRlY2F5X3BlcmlvZJR0lEtkS8hN6AOMA3NnZJRHP0BiTdLxqfxLAEsUKIwDZW52lIwGcG9saWN5lIwCZXOUjAduX3N0ZXBzlIwNbWluX3Bvb2xfc2l6ZZSMEHJlcGxheV9wb29sX3NpemWUjBR0cmFpbl9lcG9jaF9pbnRlcnZhbJSMD21heF9wYXRoX2xlbmd0aJSMFHBvbGljeV91cGRhdGVfbWV0aG9klIwUcG9saWN5X2xlYXJuaW5nX3JhdGWUjBN0YXJnZXRfbW9kZWxfdXBkYXRllIwObl9ldmFsX3NhbXBsZXOUjApiYXRjaF9zaXpllHSUdJQojAZHeW1FbnaUjBVDYXRlZ29yaWNhbE1scFFQb2xpY3mUjARzcGVjlIwCTkyUjAdyZWN0aWZ5lIwVRXBzaWxvbkdyZWVkeVN0cmF0ZWd5lIwDRFFOlIwFdHJhaW6UdJQojAFflGgZaBpoHGgbjARhbGdvlHSUjB48aXB5dGhvbi1pbnB1dC0zLThhZTFlZDJhMGI2MT6UjAhydW5fdGFza5RLDkMuAAEMAgIBAgEEAgQBCgMEARYCAgECAQIBAgECAQIBAgECAQIBAgECAQIBAgEIBJQpKXSUUpRK/////4wIX19tYWluX1+Uh5RSlH2UKIwHZ2xvYmFsc5R9lChoKYwlZHFuLnBvbGljaWVzLmNhdGVnb3JpY2FsX21scF9xX3BvbGljeZRoKZOUaC2MLmRxbi5leHBsb3JhdGlvbl9zdHJhdGVnaWVzLmVwc19ncmVlZHlfc3RyYXRlZ3mUaC2TlGgraACMCXN1YmltcG9ydJSTlIwWbGFzYWduZS5ub25saW5lYXJpdGllc5SFlFKUaC6MDWRxbi5hbGdvcy5kcW6UaC6TlGgojBJybGxhYi5lbnZzLmd5bV9lbnaUaCiTlHWMCGRlZmF1bHRzlE6MBGRpY3SUfZSMDmNsb3N1cmVfdmFsdWVzlE6MBm1vZHVsZZRoOWgPaDWMA2RvY5ROjAhxdWFsbmFtZZRoNXV0Ui4='  --variant_data 'gAN9cQBYCAAAAGV4cF9uYW1lcQFYIwAAAGV4cGVyaW1lbnRfMjAxOV8wMV8wN18xMV8xN181N18wMDAxcQJzLg=='\n"],"name":"stdout"}]},{"metadata":{"id":"3_D8rRhbjJ-G","colab_type":"text"},"cell_type":"markdown","source":["# DQN on Breakout\n","\n"]},{"metadata":{"id":"C7VujhHav6Mz","colab_type":"code","colab":{}},"cell_type":"code","source":["from dqn.envs.proxy_gym_env import ProxyGymEnv\n","from dqn.misc.retro_wrappers import wrap_deepmind_retro\n","from dqn.policies.categorical_conv_q_policy import CategoricalConvQPolicy\n","from dqn.exploration_strategies.eps_greedy_strategy import EpsilonGreedyStrategy\n","\n","from dqn.algos.dqn import DQN\n","\n","from rllab.envs.normalized_env import normalize\n","from rllab.misc.instrument import stub, run_experiment_lite\n","from rllab.exploration_strategies.ou_strategy import OUStrategy\n","from rllab.q_functions.continuous_mlp_q_function import ContinuousMLPQFunction\n","\n","def run_task(*_):\n","    game = wrap_deepmind_retro(gym.envs.make('BreakoutDeterministic-v4'))\n","\n","    env = ProxyGymEnv(game, record_video=False, record_log=False)\n","    \n","    policy = CategoricalConvQPolicy(\n","        name='dqn_policy',\n","        env_spec=env.spec,\n","        # The neural network policy should have two hidden layers, each with 32 hidden units.\n","        conv_filters=[32, 64, 64], \n","        conv_filter_sizes=[8,4,3], \n","        conv_strides=[4,2,1], \n","        conv_pads=['valid','valid','valid'],\n","        hidden_sizes=[512]\n","    )\n","    \n","    n_steps = 10000000 \n","    es = EpsilonGreedyStrategy(env_spec=env.spec, max_eps=0.5, min_eps=0.05, decay_period=n_steps//2)\n","    \n","    algo = DQN(\n","        env=env,\n","        policy=policy,\n","        es=es,\n","        n_steps=n_steps,\n","        min_pool_size         =   1000,\n","        replay_pool_size      =  50000,\n","        train_epoch_interval  =  10000,\n","        # max_path_length=np.max,\n","        policy_update_method='sgd',\n","        policy_learning_rate=0.005, # needs to be lower...\n","        target_model_update=0.5,\n","        n_eval_samples=0,\n","        batch_size=32,\n","        # Uncomment both lines (this and the plot parameter below) to enable plotting\n","        # plot=True,\n","    )\n","    algo.train()\n","\n","# run_task()\n","\n"," \n","run_experiment_lite(\n","    run_task,\n","    log_dir='./breakout_dqn',\n","    # Number of parallel workers for sampling\n","    n_parallel=1,\n","    # Only keep the snapshot parameters for the last iteration\n","    snapshot_mode=\"last\",\n","    # Specifies the seed for the experiment. If this is not provided, a random seed\n","    # will be used\n","    seed=1,\n","    # plot=True,\n",")\n","\n"],"execution_count":0,"outputs":[]}]}